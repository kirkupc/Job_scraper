{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time \n",
    "#import ast\n",
    "from all_functions import *\n",
    "import math\n",
    "import requests\n",
    "from pylatex import Document, Section, Subsection, Command\n",
    "from pylatex.utils import italic, NoEscape\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import subprocess\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_params = {\n",
    "    'results_per_site' : 10,\n",
    "    'job_title' : 'python',\n",
    "    'city' : 'worcester',\n",
    "    'site_list' : ['stack', 'indeed', 'linkedin'],\n",
    "    'linked_in_username' : 'YOUR USERNAME',\n",
    "    'linked_in_password' : 'YOUR PASSWORD',\n",
    "    'chromedriver_location' : u'chromedriverlocation',\n",
    "    'api_key' : 'YOURGOOGLEMAPSAPIKEY'\n",
    "}\n",
    "APIkey = scrap_params['api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_address(query):\n",
    "        base = 'https://maps.googleapis.com/maps/api/place/findplacefromtext/json?input='\n",
    "        locationbias= '42.3314584645106, -71.1039191294055'\n",
    "        key = APIkey\n",
    "        query = query\n",
    "        fields = 'formatted_address,name,geometry'\n",
    "        inputtype = 'textquery'\n",
    "        \n",
    "        url_search = base + query + '&inputtype=' + inputtype + '&fields=' + fields + '&locationbias=circle:2000@' + locationbias + '&key=' + key\n",
    "        my_dict = requests.get(url_search).json()\n",
    "        return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_commute(dest, origin):\n",
    "    \n",
    "    base = 'https://maps.googleapis.com/maps/api/distancematrix/json?units=imperial&'\n",
    "    origin = 'origins=' + origin + '&'\n",
    "    dest_str = 'destinations=' + dest + '&'\n",
    "    api_key = 'key=' + APIkey\n",
    "    \n",
    "    url_search = base + origin + dest_str + api_key\n",
    "    my_dict = requests.get(url_search).json()\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_duration(company_list, id_list, api_key):\n",
    "    comm_df = pd.DataFrame(columns=['company', 'place', 'address', 'distance', 'duration'])\n",
    "    for index, company in enumerate(company_list):\n",
    "        comm_dict = {}\n",
    "        company = company\n",
    "        tmp_id = id_list[int(index)]\n",
    "        text_dict = get_address(str(company))\n",
    "        if text_dict['status'] == 'ZERO_RESULTS':\n",
    "            print(company + ' has 0 results in get_address.')\n",
    "            base = 'https://maps.googleapis.com/maps/api/staticmap?'\n",
    "            place = 'None'\n",
    "            address = 'None'\n",
    "            distance = 'None'\n",
    "            duration = 'None'\n",
    "            #params = {'center' : '8 Oswald Street, Boston MA 02120' , 'zoom' : '20', 'size' : '200x200' , 'maptype' : 'roadmap', 'key': api_key}\n",
    "            params = {'center' : '8 Oswald Street, Boston MA 02120' , 'zoom' : '10', 'size' : '200x200' , 'maptype' : 'roadmap', 'key': api_key}\n",
    "            map_url = requests.get(base, params).url\n",
    "        else:\n",
    "            address = text_dict['candidates'][0]['formatted_address']\n",
    "            place = text_dict['candidates'][0]['name']    \n",
    "            res_lat_lng = str(text_dict['candidates'][0]['geometry']['location']['lat']) + ',' + str(text_dict['candidates'][0]['geometry']['location']['lng'])\n",
    "            base = 'https://maps.googleapis.com/maps/api/staticmap?'\n",
    "            #params = {'center' : '8 Oswald Street, Boston MA 02120' , 'zoom' : '18', 'size' : '200x200' , 'maptype' : 'roadmap', 'markers': 'color:blue|label:S|' + res_lat_lng, 'key': api_key} \n",
    "            params = {'center' : '8 Oswald Street, Boston MA 02120' , 'size' : '200x200' , 'maptype' : 'roadmap', 'markers': 'color:blue|label:S|' + res_lat_lng, 'key': api_key} \n",
    "            map_url = requests.get(base, params).url\n",
    "            \n",
    "            \n",
    "            comm = get_commute(dest=res_lat_lng, origin='8 Oswald Street')\n",
    "            if comm['rows'][0]['elements'][0]['status'] == 'ZERO_RESULTS':\n",
    "                print(company + ' has 0 results in get_commute.')\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    distance = (comm['rows'][0]['elements'][0]['distance']['text'])\n",
    "                    duration = (comm['rows'][0]['elements'][0]['duration']['text'])\n",
    "                except:\n",
    "                    #print(comm)\n",
    "                    break\n",
    "\n",
    "        comm_dict['company'] = company\n",
    "        comm_dict['place'] = place\n",
    "        comm_dict['address'] = address\n",
    "        comm_dict['distance'] = distance\n",
    "        comm_dict['duration'] = duration\n",
    "        comm_dict['sID'] = tmp_id\n",
    "        comm_dict['map_url'] = map_url\n",
    "        comm_tmp_df = pd.DataFrame(comm_dict, index = [comm_dict['sID']])\n",
    "        comm_df = comm_df.append(comm_tmp_df, sort=False)\n",
    "    return comm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ult_scrap(**scrap_params):\n",
    "    results = scrap_params['results_per_site']\n",
    "    job_title = scrap_params['job_title']\n",
    "    city = scrap_params['city'] \n",
    "    site_list = scrap_params['site_list']\n",
    "    linked_in_username = scrap_params['linked_in_username'] \n",
    "    linked_in_password = scrap_params['linked_in_password']\n",
    "    chromedriver_location = scrap_params['chromedriver_location'] \n",
    "    api_key = scrap_params['api_key'] \n",
    "    \n",
    "    file = 1\n",
    "    SKIPPER = 0 # Skips this many of the first jobs\n",
    "    #job_col_list = ['site', 'sID',  'title', 'company', 'location', 'salary', 'summary', 'tags','id', 'comp_loc', 'link', 'pulldate','postdate', 'ezapply']\n",
    "    #job_df = pd.DataFrame(columns=job_col_list).set_index('sID')\n",
    "    indeed_df = pd.DataFrame()\n",
    "    linkedin_df = pd.DataFrame()\n",
    "    stack_df = pd.DataFrame()\n",
    "\n",
    "    num = 0\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\") # Runs Chrome in headless mode.\n",
    "    options.add_argument('--no-sandbox') # Bypass OS security model\n",
    "    options.add_argument('--disable-gpu')  # applicable to windows os only\n",
    "    options.add_argument('start-maximized') # \n",
    "    options.add_argument('disable-infobars')\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    wd = webdriver.Chrome(chromedriver_location, options=options)\n",
    "    for site in site_list: \n",
    "        site_ind = results\n",
    "        \n",
    "        \n",
    "        if site == 'stack':\n",
    "            selen = True\n",
    "            \n",
    "            if selen == True:\n",
    "                list_o_divs = []\n",
    "                #tot_pg = ((results // 10) + 1)\n",
    "                res_left = results\n",
    "                tot_pg = ((results // 25) + 1)\n",
    "                print('stack total pages: ' + str(tot_pg))\n",
    "                #print('indeed total pages: ' + str(tot_pg))\n",
    "                for i in range(tot_pg):\n",
    "                    url = 'https://stackoverflow.com/jobs?q=python&sort=p&l=boston&d=20&u=Miles&start=' + str(i*25)\n",
    "                    #url = 'https://www.indeed.com/jobs?q=python&l=worcester&start=0&sort=date&radius=25'\n",
    "                    print(url)\n",
    "                    wd.get(url)\n",
    "                    #################### search results ####################     \n",
    "                    companies = [company.text for company in wd.find_elements_by_xpath(\"//div[@class='fc-black-700 fs-body2 -company']\")]\n",
    "                    \n",
    "                    res_len = min(len(companies), int(res_left))\n",
    "                    \n",
    "                    #l_companies = [company.text.lower().split(',')[0] for company in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//span[@class='company']\")]\n",
    "                    titles = [title.text for title in wd.find_elements_by_xpath(\"//h2[@class='fs-subheading job-details__spaced mb4']\")]\n",
    "                    tags = [tag.text for tag in wd.find_elements_by_xpath(\"//div[@class='mt12 -tag']\")]\n",
    "                    perks = [perk.text for perk in wd.find_elements_by_xpath(\"//div[@class='mt2 -perks']\")]\n",
    "                    postdates = [date.text for date in wd.find_elements_by_xpath(\"//span[contains(@class, 'ps-absolute pt2 r0 fc-black-500 fs-body1 pr12')]\")]\n",
    "                    #description = [description.text for description in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//span[@class='summary']\")]\n",
    "                    post_links = [title.get_attribute('href') for title in wd.find_elements_by_xpath(\"//h2[@class='fs-subheading job-details__spaced mb4']//a\")]\n",
    "                    #comp_links = ['https://www.indeed.com/cmp/' + str(company).replace(' ', '-') for company in companies]                    \n",
    "\n",
    "                    \n",
    "                    if res_len < 25:\n",
    "                        companies = companies[:res_len]\n",
    "                        titles = titles[:res_len]\n",
    "                        tags = tags[:res_len]\n",
    "                        perks = perks[:res_len]\n",
    "                        post_links = post_links[:res_len]\n",
    "                        postdates = postdates[:res_len]\n",
    "                    \n",
    "                    sID = [('s-ID-' + str(n)) for n in range(num, num + res_len)]\n",
    "                    pulldates = [str(time.strftime(\"%m%d%Y\", time.localtime(time.time()))) for n in range(num, num + res_len)]\n",
    "                    \n",
    "                    print(len(titles), len(companies), len(post_links), len(postdates), len(pulldates))\n",
    "                    #return 'bitch'\n",
    "                    stack_df = pd.DataFrame({'ID' : sID, 'title' : titles, 'company' : companies, 'post_link' : post_links, 'postdate' : postdates, 'pulldate' : pulldates}) # functional\n",
    "                    #return list_df\n",
    "            if selen == False:\n",
    "                site_ind = 0 # set to 0 at start of site and compared to results so there are 'results' number of pulls per site\n",
    "                list_o_divs = [] # holds divs if multiple pages\n",
    "                tmp_dict = {} # holds info on each div loop\n",
    "                # https://stackoverflow.com/jobs?q=data+scientist&l=boston&d=20&u=Miles\n",
    "                pg = 1 # page counter \n",
    "                base = 'https://stackoverflow.com/jobs?'\n",
    "                tot_pg = ((results // 20) + 1)\n",
    "                print('Stack overflow total pages: ' + str(tot_pg))\n",
    "                while pg <= tot_pg: \n",
    "                    params = {'q' : job_title, 'sort' : 'p', 'l' : city, 'd' : '20', 'u': 'Miles', 'pg': pg} \n",
    "                    page = requests.get(base, params) \n",
    "                    print(page.url)\n",
    "                    time.sleep(1)\n",
    "                    soup = get_soup(page.text)\n",
    "                    divs = soup.find_all(name=\"div\", attrs={\"class\":\"-job-summary\"})\n",
    "                    list_o_divs.append(divs)\n",
    "                    pg = pg + 1\n",
    "                for divs in list_o_divs:\n",
    "                    for div in divs:\n",
    "                        if (site_ind < results): # if there is a div past the number of results for the site, doesn't add it to tmp_dict\n",
    "                            tmp_dict['sID'] = 'ID' + str(num)\n",
    "                            tmp_dict['site'] = site\n",
    "                            tmp_dict['pulldate'] = str(time.strftime(\"%m-%d-%Y_%I-%M_%p\", time.localtime(time.time())))\n",
    "                            tmp_dict['link'] = 'https://www.stackoverflow.com' + div.find(name='a', attrs={'class':'s-link'})['href']\n",
    "                            # tmp_dict['link'] = extract_link(div, site)\n",
    "                            tmp_dict['title'] = div.find(name='a', attrs={'class':'s-link'})['title']\n",
    "                            # tmp_dict['title'] = extract_job_title(div, site)\n",
    "                            tmp_dict['summary'] = 'No Stack Summary yet'\n",
    "                            if div.find('div', attrs={'class': '-perks'}).text is None:\n",
    "                                tmp_dict['salary'] = 'NA'\n",
    "                            else:\n",
    "                                div.find('div', attrs={'class': '-perks'}).text.replace('\\n', ' ').replace('\\r', '').strip()\n",
    "                            # tmp_dict['salary'] = extract_salary(div, site)\n",
    "                            tmp_dict['location'] = div.find_all('div', attrs={'class': '-company'}).split('-')[1].text\n",
    "                            # tmp_dict['location'] = extract_location(div, site).strip()\n",
    "                            tmp_dict['tags'] = ', '.join(div.find_all('a', attrs={'class': 'post-tag job-link no-tag-menu'}))\n",
    "                            # tmp_dict['tags'] = extract_tags(div, site)\n",
    "                            tmp_dict['id'] = div.find('span', attrs={'class': 'fav-toggle'})['data-jobid']\n",
    "                            #tmp_dict['id'] = extract_id(div, site)\n",
    "                            tmp_dict['ezapply'] = 'No Easy Applications on Stack'\n",
    "                            if div.find('span', attrs={'class': 'ps-absolute pt2 r0 fc-black-500 fs-body1 pr12 t24'}).text is None:\n",
    "                                tmp_dict['postdate'] = div.find('span', attrs={'class': 'ps-absolute pt2 r0 fc-black-500 fs-body1 pr12 t32'}).text\n",
    "                            else:\n",
    "                                tmp_dict['postdate'] = div.find('span', attrs={'class': 'ps-absolute pt2 r0 fc-black-500 fs-body1 pr12 t24'}).text\n",
    "                            # tmp_dict['postdate'] = extract_postdate(div, site)\n",
    "                            tmp_dict['company'] = div.find_all(name=\"div\", attrs={\"class\":\"-company\"}).split('-')[0].text.replace('\\n', '').replace('\\r','')\n",
    "                            #tmp_dict['company'] = extract_company(div, site).split('-')[0].strip()\n",
    "                            tmp_dict['comp_loc'] = div.find_all('div', attrs={'class': '-company'}).text\n",
    "                            tmp_df = pd.DataFrame(tmp_dict, index=[tmp_dict['sID']])\n",
    "                            job_df = job_df.append(tmp_df, sort=False)\n",
    "                            num = num + 1\n",
    "                            site_ind = site_ind + 1\n",
    "            \n",
    "        if site == 'indeed':\n",
    "            \n",
    "            selen = True \n",
    "            \n",
    "            if selen == True:\n",
    "                list_o_divs = []\n",
    "                #tot_pg = ((results // 10) + 1)\n",
    "                res_left = results\n",
    "                tot_pg = ((results // 10) + 1)\n",
    "                print('indeed total pages: ' + str(tot_pg))\n",
    "                #print('indeed total pages: ' + str(tot_pg))\n",
    "                for i in range(tot_pg):\n",
    "                    url = 'https://www.indeed.com/jobs?q=' + job_title + '&l=' + city + '&start=' + str(i*10) + '&sort=' + 'date' + '&radius=' + '25'\n",
    "                    #url = 'https://www.indeed.com/jobs?q=python&l=worcester&start=0&sort=date&radius=25'\n",
    "                    print(url)\n",
    "                    wd.get(url)\n",
    "                    #################### search results ####################     \n",
    "                    companies = [company.text for company in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//span[@class='company']\")]\n",
    "                    \n",
    "                    res_len = min(len(companies), int(res_left))\n",
    "                    \n",
    "                    l_companies = [company.text.lower().split(',')[0] for company in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//span[@class='company']\")]\n",
    "                    titles = [title.text for title in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//h2[@class='jobtitle']\")]\n",
    "                    description = [description.text for description in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//span[@class='summary']\")]\n",
    "                    post_links = [post_link.get_attribute('href') for post_link in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//h2[@class='jobtitle']//a\")]\n",
    "                    comp_links = ['https://www.indeed.com/cmp/' + str(company).replace(' ', '-') for company in companies]                    \n",
    "                    \n",
    "                    if res_len < 25:\n",
    "                        l_companies = l_companies[:res_len]\n",
    "                        titles = titles[:res_len]\n",
    "                        description = description[:res_len]\n",
    "                        companies = companies[:res_len]\n",
    "                        post_links = post_links[:res_len]\n",
    "                        comp_links = comp_links[:res_len]\n",
    "                    \n",
    "                    sID = [('i-ID-' + str(n)) for n in range(num, num + res_len)]\n",
    "                    pulldates = [str(time.strftime(\"%m%d%Y\", time.localtime(time.time()))) for n in range(num, num + res_len)]\n",
    "                    \n",
    "                    list_df = pd.DataFrame({'ID' : sID, 'title' : titles, 'company' : companies, 'summary' : description, 'post_link' : post_links, 'comp_link' : comp_links, 'l_company' : l_companies, 'pulldate' : pulldates}) # functional\n",
    "                    list_df['ID'] = list_df['ID'].astype('str')\n",
    "                    \n",
    "                    #################### comp list ####################     \n",
    "                    comp_list = []\n",
    "                    for index, link in enumerate(comp_links):\n",
    "                        index = sID[index]\n",
    "                        wd.get(link)\n",
    "                        try:\n",
    "                            company_name = wd.find_element_by_xpath(\"//div[@class='cmp-company-name-container']\").text\n",
    "                        except:\n",
    "                            comp_list.append(['No company name found', '', '', '', '', ''])\n",
    "                            continue\n",
    "                        l_company_name = wd.find_element_by_xpath(\"//div[@class='cmp-company-name-container']\").text.lower().split(',')[0]\n",
    "                        try:\n",
    "                            average_comp_rating = wd.find_element_by_xpath(\"//span[@class='cmp-header-rating-average']\").text\n",
    "                        except:\n",
    "                            average_comp_rating = 'Not ranked.'\n",
    "                        salaries_link = 'https://www.indeed.com/cmp/' + company_name + '/salaries/'\n",
    "                        reviews_link = 'https://www.indeed.com/cmp/' + company_name + '/reviews/'\n",
    "                        comp_list.append([str(index), company_name, average_comp_rating, salaries_link, reviews_link, l_company_name])\n",
    "                    comp_df = pd.DataFrame(comp_list, columns=['ID', 'company', 'rating', 'salaries_link', 'reviews_link', 'l_company'])\n",
    "\n",
    "                    #################### Post list ####################     \n",
    "                    post_list = []\n",
    "                    for index, link in enumerate(post_links):\n",
    "                        index = sID[index]\n",
    "                        wd.get(link)\n",
    "                        jobtitle = wd.find_element_by_xpath(\"//h3[contains(@class, 'jobsearch-JobInfoHeader-title')]\").text\n",
    "                        try:\n",
    "                            company_pic = wd.find_element_by_xpath(\"//img[@class='jobsearch-CompanyAvatar-image']\").get_attribute('src')\n",
    "                        except:\n",
    "                            company_pic = 'NA'\n",
    "                        line = wd.find_element_by_xpath(\"//div[contains(@class, 'jobsearch-InlineCompanyRating')]\").text\n",
    "                        location = line.split('\\n')[-1]\n",
    "                        company_name = line.split('\\n')[0]\n",
    "                        l_company_name = company_name.lower().split(',')[0]\n",
    "                        comp_loc = company_name + ', ' + location\n",
    "                        body = wd.find_element_by_xpath(\"//div[contains(@class, 'jobsearch-JobComponent-description')]\").text\n",
    "                        post_time = wd.find_element_by_xpath(\"//div[@class='jobsearch-JobMetadataFooter']\").text.split(' - ')[1].strip()\n",
    "                        post_list.append([str(index), jobtitle, company_name, comp_loc, post_time, location, company_pic, body, l_company_name])\n",
    "                        \n",
    "                    post_df = pd.DataFrame(post_list, columns=['ID', 'title', 'company', 'comp_loc', 'postdate', 'location', 'company_pic', 'body_text', 'l_company'])\n",
    "                    \n",
    "                    #################### DF merging ####################                    \n",
    "                    post_comp_df = comp_df.merge(post_df, on=['ID'], how='right').drop(columns=['l_company_y', 'company_y']).rename({'company_x' : 'company', 'l_company_x' : 'l_company'}, axis='columns')\n",
    "                    post_comp_df['ID'] = post_comp_df['ID'].astype('str')\n",
    "                    tmp_df = post_comp_df.merge(list_df, on=['ID'], how='outer').drop(columns=['l_company_y', 'company_y', 'title_y']).rename({'company_x' : 'company', 'l_company_x' : 'l_company', 'title_x' : 'title'}, axis='columns')\n",
    "                    indeed_df = indeed_df.append(tmp_df, sort=False)\n",
    "                    \n",
    "                    num += len(tmp_df)\n",
    "                    res_left = res_left - len(tmp_df)\n",
    "            if selen == False:\n",
    "                    pass\n",
    "                    # code is below             \n",
    "                    \n",
    "                    \n",
    "        if site == 'linkedin':\n",
    "            \n",
    "            tot_pg = ((results // 25) + 1)\n",
    "            res_left = results\n",
    "            print('linkedin total pages: ' + str(tot_pg))\n",
    "            login_url = 'https://www.linkedin.com/'\n",
    "            wd.get(login_url)\n",
    "            #wd.find_element_by_xpath(\"//a[@class='sign-in-link']\").click\n",
    "            #wd.find_element_by_xpath(\"//a[@class='form-toggle']\").click\n",
    "            login_email = wd.find_element_by_id(\"login-email\")\n",
    "            login_email.send_keys(linked_in_username)\n",
    "            login_pass = wd.find_element_by_id(\"login-password\")\n",
    "            login_pass.send_keys(linked_in_password)\n",
    "            wd.find_element_by_id(\"login-submit\").click\n",
    "            #cookies = wd.get_cookie('bcookie')\n",
    "            \n",
    "            for i in range(tot_pg):\n",
    "                url = 'http://www.linkedin.com/jobs/search/?keywords=data+science&location=Boston&sortBy=DD&start=' + str(i*25)\n",
    "                print('list url : ', url)\n",
    "                wd.get(url)\n",
    "                \n",
    "                listings = wd.find_elements_by_xpath(\"//li[@class='job-listing']\")\n",
    "                res_len = min(len(listings), int(res_left))\n",
    "                \n",
    "                post_link_base = 'https://www.linkedin.com/uas/login?session_redirect='\n",
    "                post_links = [l.get_attribute('href') for l in wd.find_elements_by_xpath(\"//a[@class='job-title-link']\")]\n",
    "                comp_links = []\n",
    "                for elem in wd.find_elements_by_xpath(\"//div[@class='company-name']\"):\n",
    "                    if elem.find_element_by_xpath(\"//a\") == None:\n",
    "                        comp_links.append('NA')\n",
    "                    else:\n",
    "                        comp_links.append(elem.find_element_by_xpath(\"//a\").get_attribute('href'))\n",
    "                #comp_links = [l.get_attribute('href') for l in wd.find_elements_by_xpath(\"//a[@class='company-name-link']\")]\n",
    "                titles = [t.text for t in wd.find_elements_by_xpath(\"//span[@class='job-title-text']\")]\n",
    "                locations = [L.text for L in wd.find_elements_by_xpath(\"//span[@class='job-location']/span\")]\n",
    "                companies = [c.text for c in wd.find_elements_by_xpath(\"//span[@class='company-name-text']\")]\n",
    "                descr = [d.text for d in wd.find_elements_by_xpath(\"//div[@class='job-description']\")]\n",
    "                ezapply = [e.text for e in wd.find_elements_by_xpath(\"//div[@class='job-flavor-in-apply-container']\")]\n",
    "                \n",
    "                company_pics = [i.get_attribute('src') for i in wd.find_elements_by_xpath(\"//a[@class='company-logo-link']//img\")]\n",
    "                \n",
    "                \n",
    "                if res_len < 25:\n",
    "                    post_links = post_links[:res_len]\n",
    "                    comp_links = comp_links[:res_len]\n",
    "                    titles = titles[:res_len]\n",
    "                    locations = locations[:res_len]\n",
    "                    companies = companies[:res_len]\n",
    "                    descr = descr[:res_len]\n",
    "                    ezapply = ezapply[:res_len]\n",
    "                    company_pics = company_pics[:res_len]\n",
    "                sID = [('L-ID-' + str(n)) for n in range(results-res_left, res_left)]\n",
    "                pulldates = [str(time.strftime(\"%m%d%Y\", time.localtime(time.time()))) for n in range(num, num + res_len)]\n",
    "                comp_loc = []\n",
    "                for i, elem in enumerate(companies):\n",
    "                    comp_loc_str = (elem.strip() + ', ' + locations[i].strip())\n",
    "                    comp_loc.append(comp_loc_str)\n",
    "                try:\n",
    "                    tmp_df = pd.DataFrame({'ID': sID, 'title' : titles, 'company' : companies, 'location' : locations, \n",
    "                                       'post_link' : post_links, 'comp_link' : comp_links, 'ezapply' : ezapply, 'company_pic' : company_pics, \n",
    "                                       'pulldate' : pulldates, 'comp_loc': comp_loc})\n",
    "                except:\n",
    "                    print(len(titles), len(comp_loc), len(companies), len(locations), len(post_links), len(comp_links), len(ezapply), len(company_pics), len(pulldates))\n",
    "                    return 'bitch'\n",
    "\n",
    "                \n",
    "                #list_df = pd.DataFrame({'index' : sID, 'title' : titles, 'company' : companies, 'summary' : description, 'post_links' : post_links, 'comp_links' : comp_links, 'l_company' : l_companies}) # functional\n",
    "                res_left = res_left - len(tmp_df)\n",
    "                \n",
    "                                    #################### DF merging ####################\n",
    "                    \n",
    "                #post_comp_df = comp_df.merge(post_df, on=['index'], how='right').drop(columns=['l_company_y', 'company_y']).rename({'company_x' : 'company', 'l_company_x' : 'l_company'}, axis='columns')\n",
    "                #post_comp_df['index'] = post_comp_df['index'].astype('str')\n",
    "                #merge_df = post_df.merge(tmp_df, on=['index'], how='outer').drop(columns=['l_company_y', 'company_y', 'title_y']).rename({'company_x' : 'company', 'l_company_x' : 'l_company', 'title_x' : 'title'}, axis='columns')\n",
    "                linkedin_df = linkedin_df.append(tmp_df, sort=False)\n",
    "    \n",
    "    job_df = linkedin_df.append(indeed_df, sort=False)\n",
    "    job_df = job_df.append(stack_df, sort=False)\n",
    "    #companies = job_df['comp_loc']\n",
    "    #ids = job_df.index.tolist()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #comm_df = dist_duration(companies, ids, api_key)\n",
    "\n",
    "    #fin_df = job_df.merge(comm_df, how = 'left', left_index=True, right_index=True)\n",
    "    clean_df = False\n",
    "    \n",
    "    if clean_df == True:\n",
    "        #fin_df.rename({'company_x' : 'company'}, axis='columns', inplace=True)\n",
    "        #fin_df.drop(['sID_x', 'sID_y', 'company_y'], axis='columns', inplace=True)\n",
    "        #nice_col_list = ['site', 'title', 'comp_loc', 'distance', 'duration', 'summary', 'tags', 'link', 'ezapply']\n",
    "        #nice_df = fin_df[nice_col_list]\n",
    "        \n",
    "        return clean_df\n",
    "    file = file + 1\n",
    "    wd.quit()\n",
    "    output_dict = {'job_title' : job_title, 'city' : city}\n",
    "    return job_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linkedin total pages: 1\n",
      "list url :  http://www.linkedin.com/jobs/search/?keywords=data+science&location=Boston&sortBy=DD&start=0\n",
      "stack total pages: 1\n",
      "https://stackoverflow.com/jobs?q=python&sort=p&l=boston&d=20&u=Miles&start=0\n",
      "15 15 15 15 15\n",
      "indeed total pages: 2\n",
      "https://www.indeed.com/jobs?q=python&l=boston&start=0&sort=date&radius=25\n",
      "https://www.indeed.com/jobs?q=python&l=boston&start=10&sort=date&radius=25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>post_link</th>\n",
       "      <th>comp_link</th>\n",
       "      <th>ezapply</th>\n",
       "      <th>company_pic</th>\n",
       "      <th>pulldate</th>\n",
       "      <th>comp_loc</th>\n",
       "      <th>rating</th>\n",
       "      <th>salaries_link</th>\n",
       "      <th>reviews_link</th>\n",
       "      <th>l_company</th>\n",
       "      <th>postdate</th>\n",
       "      <th>body_text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L-ID-0</td>\n",
       "      <td>Technical Delivery Manager</td>\n",
       "      <td>Appian</td>\n",
       "      <td>Boston, MA, US</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/technical-d...</td>\n",
       "      <td>https://www.linkedin.com/jobs?trk=jobs_chrome_...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.linkedin.com/scds/common/u/images/...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Appian, Boston, MA, US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L-ID-1</td>\n",
       "      <td>Sr UI Angular 2 Developer - US Citizen/Green c...</td>\n",
       "      <td>Financial firm</td>\n",
       "      <td>Boston, MA, US</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/sr-ui-angul...</td>\n",
       "      <td>https://www.linkedin.com/jobs?trk=jobs_chrome_...</td>\n",
       "      <td>Easy Apply</td>\n",
       "      <td>https://www.linkedin.com/scds/common/u/images/...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Financial firm, Boston, MA, US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L-ID-2</td>\n",
       "      <td>VP of Product Management</td>\n",
       "      <td>Quest Groups</td>\n",
       "      <td>Greater Boston Area</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/vp-of-produ...</td>\n",
       "      <td>https://www.linkedin.com/jobs?trk=jobs_chrome_...</td>\n",
       "      <td>Easy Apply</td>\n",
       "      <td>https://www.linkedin.com/scds/common/u/images/...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Quest Groups, Greater Boston Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L-ID-3</td>\n",
       "      <td>Technical Associate I, AgeLab - Cambridge</td>\n",
       "      <td>Massachusetts Institute of Technology</td>\n",
       "      <td>Cambridge, MA, US</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/technical-a...</td>\n",
       "      <td>https://www.linkedin.com/jobs?trk=jobs_chrome_...</td>\n",
       "      <td></td>\n",
       "      <td>https://media.licdn.com/dms/image/C4E0BAQHBK7q...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Massachusetts Institute of Technology, Cambrid...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L-ID-4</td>\n",
       "      <td>Product Manager -</td>\n",
       "      <td>Casenet, LLC</td>\n",
       "      <td>Bedford, MA, US</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/product-man...</td>\n",
       "      <td>https://www.linkedin.com/jobs?trk=jobs_chrome_...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Casenet, LLC, Bedford, MA, US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L-ID-5</td>\n",
       "      <td>Engineer l</td>\n",
       "      <td>Weston &amp; Sampson</td>\n",
       "      <td>Foxborough, MA, US</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/engineer-l-...</td>\n",
       "      <td>https://www.linkedin.com/jobs?trk=jobs_chrome_...</td>\n",
       "      <td></td>\n",
       "      <td>https://media.licdn.com/dms/image/C4D0BAQFbppI...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Weston &amp; Sampson, Foxborough, MA, US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>L-ID-6</td>\n",
       "      <td>Senior Tech Lead</td>\n",
       "      <td>Trianz</td>\n",
       "      <td>Manchester, NH, US</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-tech...</td>\n",
       "      <td>https://www.linkedin.com/jobs?trk=jobs_chrome_...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Trianz, Manchester, NH, US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L-ID-7</td>\n",
       "      <td>Healthcare Data Engineer</td>\n",
       "      <td>iSpecimen, Inc.</td>\n",
       "      <td>Boston, MA, US</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/healthcare-...</td>\n",
       "      <td>https://www.linkedin.com/jobs?trk=jobs_chrome_...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>11082018</td>\n",
       "      <td>iSpecimen, Inc., Boston, MA, US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L-ID-8</td>\n",
       "      <td>QA Automation Engineer w/ Innovative Marketing...</td>\n",
       "      <td>Workbridge Associates</td>\n",
       "      <td>Boston, MA, US</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/qa-automati...</td>\n",
       "      <td>https://www.linkedin.com/jobs?trk=jobs_chrome_...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Workbridge Associates, Boston, MA, US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L-ID-9</td>\n",
       "      <td>Senior Sales Operations Planning Analyst* - Fa...</td>\n",
       "      <td>Acushnet Company</td>\n",
       "      <td>Fairhaven, MA, US</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/senior-sale...</td>\n",
       "      <td>https://www.linkedin.com/jobs?trk=jobs_chrome_...</td>\n",
       "      <td></td>\n",
       "      <td>https://media.licdn.com/dms/image/C4E0BAQE8M_T...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Acushnet Company, Fairhaven, MA, US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>L-ID-10</td>\n",
       "      <td>Lead Data Manager- Home Based Contract</td>\n",
       "      <td>Premier Research</td>\n",
       "      <td>United States</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/lead-data-m...</td>\n",
       "      <td>https://www.linkedin.com/jobs?trk=jobs_chrome_...</td>\n",
       "      <td></td>\n",
       "      <td>https://media.licdn.com/dms/image/C560BAQHEqcj...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Premier Research, United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>L-ID-11</td>\n",
       "      <td>Product Marketing Manager</td>\n",
       "      <td>HireMinds</td>\n",
       "      <td>Greater Boston Area</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/product-mar...</td>\n",
       "      <td>https://www.linkedin.com/jobs?trk=jobs_chrome_...</td>\n",
       "      <td></td>\n",
       "      <td>https://media.licdn.com/dms/image/C510BAQHVUFQ...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>HireMinds, Greater Boston Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>L-ID-12</td>\n",
       "      <td>Director of Data Science -</td>\n",
       "      <td>Workbridge Associates</td>\n",
       "      <td>Boston, MA, US</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/director-of...</td>\n",
       "      <td>https://www.linkedin.com/jobs?trk=jobs_chrome_...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Workbridge Associates, Boston, MA, US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>L-ID-13</td>\n",
       "      <td>Java Engineer - Boston</td>\n",
       "      <td>Workbridge Associates</td>\n",
       "      <td>Boston, MA, US</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/java-engine...</td>\n",
       "      <td>https://www.linkedin.com/jobs?trk=jobs_chrome_...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Workbridge Associates, Boston, MA, US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>L-ID-14</td>\n",
       "      <td>Capital Assets Program Manager</td>\n",
       "      <td>Harvard University</td>\n",
       "      <td>Cambridge, Massachusetts, United States</td>\n",
       "      <td>https://www.linkedin.com/jobs/view/capital-ass...</td>\n",
       "      <td>https://www.linkedin.com/jobs?trk=jobs_chrome_...</td>\n",
       "      <td></td>\n",
       "      <td>https://media.licdn.com/dms/image/C4E0BAQF5t62...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Harvard University, Cambridge, Massachusetts, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i-ID-0</td>\n",
       "      <td>Director of Operations &amp; Security</td>\n",
       "      <td>SessionM</td>\n",
       "      <td>Boston, MA 02210</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=83ed9b84acbaa...</td>\n",
       "      <td>https://www.indeed.com/cmp/SessionM,-Inc.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>11082018</td>\n",
       "      <td>SessionM, Inc., Boston, MA 02210</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://www.indeed.com/cmp/SessionM/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/SessionM/reviews/</td>\n",
       "      <td>sessionm</td>\n",
       "      <td>5 hours ago</td>\n",
       "      <td>Contract\\nAs the Director of Operations &amp; Secu...</td>\n",
       "      <td>Highly engaged technologist with broad experie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i-ID-1</td>\n",
       "      <td>Sr Backend Engineer (Python)</td>\n",
       "      <td>Strategic Employment Partners</td>\n",
       "      <td>Cambridge, MA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=1a3b2a17d3718...</td>\n",
       "      <td>https://www.indeed.com/cmp/Strategic-Employmen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d2q79iu7y748jz.cloudfront.net/s/_logo/...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Strategic Employment Partners, Cambridge, MA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://www.indeed.com/cmp/Strategic Employmen...</td>\n",
       "      <td>https://www.indeed.com/cmp/Strategic Employmen...</td>\n",
       "      <td>strategic employment partners</td>\n",
       "      <td>3 hours ago</td>\n",
       "      <td>$120,000 a year\\nSenior Backend Engineer (Pyth...</td>\n",
       "      <td>5+ years working with Python professionally. W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i-ID-2</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>XL Fleet</td>\n",
       "      <td>Brighton, MA 02135</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=5a560fd3d8cec...</td>\n",
       "      <td>https://www.indeed.com/cmp/XL-Fleet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>11082018</td>\n",
       "      <td>XL Fleet, Brighton, MA 02135</td>\n",
       "      <td>Not ranked.</td>\n",
       "      <td>https://www.indeed.com/cmp/XL Fleet/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/XL Fleet/reviews/</td>\n",
       "      <td>xl fleet</td>\n",
       "      <td>5 hours ago</td>\n",
       "      <td>XL seeks an experienced Data Engineer to work ...</td>\n",
       "      <td>Including object-oriented Python as well as fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i-ID-3</td>\n",
       "      <td>INTERNSHIP: R&amp;I Reporting and Analytics Intern</td>\n",
       "      <td>TIAA</td>\n",
       "      <td>Waltham, MA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=6a679ac30cded...</td>\n",
       "      <td>https://www.indeed.com/cmp/TIAA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d2q79iu7y748jz.cloudfront.net/s/_logo/...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>TIAA, Waltham, MA</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://www.indeed.com/cmp/TIAA/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/TIAA/reviews/</td>\n",
       "      <td>tiaa</td>\n",
       "      <td>4 hours ago</td>\n",
       "      <td>Internship\\nJob Description\\nCOMPANY OVERVIEW:...</td>\n",
       "      <td>Experience with programming language (SQL, SAS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i-ID-4</td>\n",
       "      <td>Sr. Data Scientist</td>\n",
       "      <td>Celgene Corporation</td>\n",
       "      <td>Cambridge, MA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=ac24bedf9e2a3...</td>\n",
       "      <td>https://www.indeed.com/cmp/Celgene</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d2q79iu7y748jz.cloudfront.net/s/_logo/...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Celgene, Cambridge, MA</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.indeed.com/cmp/Celgene Corporation...</td>\n",
       "      <td>https://www.indeed.com/cmp/Celgene Corporation...</td>\n",
       "      <td>celgene corporation</td>\n",
       "      <td>4 hours ago</td>\n",
       "      <td>Other Locations: US- MA- Cambridge\\n\\nDescript...</td>\n",
       "      <td>Excellent skills in R programming and experien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i-ID-5</td>\n",
       "      <td>Principal Big Data Engineer</td>\n",
       "      <td>Fidelity Investments</td>\n",
       "      <td>Boston, MA 02212</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=01e8cdaf09531...</td>\n",
       "      <td>https://www.indeed.com/cmp/Fidelity-Investments</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d2q79iu7y748jz.cloudfront.net/s/_logo/...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Fidelity Investments, Boston, MA 02212</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://www.indeed.com/cmp/Fidelity Investment...</td>\n",
       "      <td>https://www.indeed.com/cmp/Fidelity Investment...</td>\n",
       "      <td>fidelity investments</td>\n",
       "      <td>5 hours ago</td>\n",
       "      <td>Fidelity Personal Investing has an opportunity...</td>\n",
       "      <td>Your expertise in languages such as Python, Ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i-ID-6</td>\n",
       "      <td>Lead software engineer in cloud-based data man...</td>\n",
       "      <td>hits</td>\n",
       "      <td>Boston, MA 02115</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=d5a7067002a0d...</td>\n",
       "      <td>https://www.indeed.com/cmp/HiTS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NA</td>\n",
       "      <td>11082018</td>\n",
       "      <td>HiTS, Boston, MA 02115</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://www.indeed.com/cmp/hits/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/hits/reviews/</td>\n",
       "      <td>hits</td>\n",
       "      <td>5 hours ago</td>\n",
       "      <td>About the position\\nThe Harvard Medical School...</td>\n",
       "      <td>Python, JavaScript and Java; High fluency in m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i-ID-7</td>\n",
       "      <td>Sr. Data Science Engineer</td>\n",
       "      <td>Staples</td>\n",
       "      <td>Framingham, MA 01702</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=6db347a6afe08...</td>\n",
       "      <td>https://www.indeed.com/cmp/Staples</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d2q79iu7y748jz.cloudfront.net/s/_logo/...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Staples, Framingham, MA 01702</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://www.indeed.com/cmp/Staples/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/Staples/reviews/</td>\n",
       "      <td>staples</td>\n",
       "      <td>4 hours ago</td>\n",
       "      <td>Description\\nWe are looking for an experienced...</td>\n",
       "      <td>With at least a working knowledge of Python. P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i-ID-8</td>\n",
       "      <td>Principal Data Science Engineer</td>\n",
       "      <td>Staples</td>\n",
       "      <td>Framingham, MA 01702</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=21f4b09968b51...</td>\n",
       "      <td>https://www.indeed.com/cmp/Staples</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d2q79iu7y748jz.cloudfront.net/s/_logo/...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Staples, Framingham, MA 01702</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://www.indeed.com/cmp/Staples/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/Staples/reviews/</td>\n",
       "      <td>staples</td>\n",
       "      <td>4 hours ago</td>\n",
       "      <td>Description\\nWe are looking for an experienced...</td>\n",
       "      <td>With at least a working knowledge of Python. E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i-ID-9</td>\n",
       "      <td>Language Engineer</td>\n",
       "      <td>Amazon.com</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=04f5f15d9d04b...</td>\n",
       "      <td>https://www.indeed.com/cmp/Amazon.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d2q79iu7y748jz.cloudfront.net/s/_logo/...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Amazon.com, Boston, MA</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://www.indeed.com/cmp/Amazon.com/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/Amazon.com/reviews/</td>\n",
       "      <td>amazon.com</td>\n",
       "      <td>7 hours ago</td>\n",
       "      <td>Job Description\\n\\nInterested in Amazon Echo? ...</td>\n",
       "      <td>Experience in Perl, Python, or another scripti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i-ID-10</td>\n",
       "      <td>NLP Applied Scientist</td>\n",
       "      <td>Amazon.com</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=50b51e203d80d...</td>\n",
       "      <td>https://www.indeed.com/cmp/Amazon.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d2q79iu7y748jz.cloudfront.net/s/_logo/...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Amazon.com, Boston, MA</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://www.indeed.com/cmp/Amazon.com/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/Amazon.com/reviews/</td>\n",
       "      <td>amazon.com</td>\n",
       "      <td>7 hours ago</td>\n",
       "      <td>Job Description\\nInterested in Amazon Echo and...</td>\n",
       "      <td>Expert in at least one more major programming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i-ID-11</td>\n",
       "      <td>Scientist I, Biomedical Informatics</td>\n",
       "      <td>Biogen</td>\n",
       "      <td>Cambridge, MA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=2c1ed88e515b9...</td>\n",
       "      <td>https://www.indeed.com/cmp/Biogen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d2q79iu7y748jz.cloudfront.net/s/_logo/...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Biogen, Cambridge, MA</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://www.indeed.com/cmp/Biogen/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/Biogen/reviews/</td>\n",
       "      <td>biogen</td>\n",
       "      <td>6 hours ago</td>\n",
       "      <td>Effective use of disruptive technologies is at...</td>\n",
       "      <td>Expertise in developing, using or adapting ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i-ID-12</td>\n",
       "      <td>Consultant - Supply Chain Analytics</td>\n",
       "      <td>McKinsey &amp; Company</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=d7e575de92ecc...</td>\n",
       "      <td>https://www.indeed.com/cmp/McKinsey-&amp;-Company</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d2q79iu7y748jz.cloudfront.net/s/_logo/...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>McKinsey &amp; Company, Boston, MA</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://www.indeed.com/cmp/McKinsey &amp; Company/...</td>\n",
       "      <td>https://www.indeed.com/cmp/McKinsey &amp; Company/...</td>\n",
       "      <td>mckinsey &amp; company</td>\n",
       "      <td>9 hours ago</td>\n",
       "      <td>Qualifications\\nMBA or advanced degree in Oper...</td>\n",
       "      <td>R, Python, SAS, SQL, Tableau, etc. MBA or adva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i-ID-13</td>\n",
       "      <td>Manager, Software Development – Alexa – Cambri...</td>\n",
       "      <td>Amazon.com</td>\n",
       "      <td>Cambridge, MA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=1fe6e6f85af19...</td>\n",
       "      <td>https://www.indeed.com/cmp/Amazon.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d2q79iu7y748jz.cloudfront.net/s/_logo/...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Amazon.com, Cambridge, MA</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://www.indeed.com/cmp/Amazon.com/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/Amazon.com/reviews/</td>\n",
       "      <td>amazon.com</td>\n",
       "      <td>8 hours ago</td>\n",
       "      <td>Job Description\\nDo you want to work on Alexa,...</td>\n",
       "      <td>Experience with one or more modern programming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i-ID-14</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Amazon.com</td>\n",
       "      <td>North Reading, MA</td>\n",
       "      <td>https://www.indeed.com/rc/clk?jk=42650568a6466...</td>\n",
       "      <td>https://www.indeed.com/cmp/Amazon.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d2q79iu7y748jz.cloudfront.net/s/_logo/...</td>\n",
       "      <td>11082018</td>\n",
       "      <td>Amazon.com, North Reading, MA</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://www.indeed.com/cmp/Amazon.com/salaries/</td>\n",
       "      <td>https://www.indeed.com/cmp/Amazon.com/reviews/</td>\n",
       "      <td>amazon.com</td>\n",
       "      <td>7 hours ago</td>\n",
       "      <td>Job Description\\nAre you inspired by invention...</td>\n",
       "      <td>Proficiency in, at least, one modern programmi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s-ID-0</td>\n",
       "      <td>Senior/Principal Software Engineers (Backend/D...</td>\n",
       "      <td>Kyruus - Boston, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stackoverflow.com/jobs/204101/senior-p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11082018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1w ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s-ID-1</td>\n",
       "      <td>Systems Engineer</td>\n",
       "      <td>Amadeus North America, Inc. - Waltham, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stackoverflow.com/jobs/207582/systems-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11082018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20h ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s-ID-2</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>REsurety - Boston, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stackoverflow.com/jobs/201669/data-eng...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11082018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3d ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s-ID-3</td>\n",
       "      <td>Deep Learning Engineer</td>\n",
       "      <td>DeepCure - Boston, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stackoverflow.com/jobs/207253/deep-lea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11082018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5d ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s-ID-4</td>\n",
       "      <td>Python Developer</td>\n",
       "      <td>Mide Technology - Medford, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stackoverflow.com/jobs/204413/python-d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11082018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5d ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>s-ID-5</td>\n",
       "      <td>Senior Software Engineer</td>\n",
       "      <td>Vista Higher Learning - Boston, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stackoverflow.com/jobs/207206/senior-s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11082018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6d ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>s-ID-6</td>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>CarGurus - Cambridge, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stackoverflow.com/jobs/204279/senior-d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11082018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6d ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>s-ID-7</td>\n",
       "      <td>Senior Software Engineer, Backend/API</td>\n",
       "      <td>PathAI - Boston, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stackoverflow.com/jobs/169118/senior-s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11082018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1w ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>s-ID-8</td>\n",
       "      <td>Software Engineering Manager, Backend/API</td>\n",
       "      <td>PathAI - Boston, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stackoverflow.com/jobs/169119/software...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11082018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1w ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>s-ID-9</td>\n",
       "      <td>Senior/Principal Software Engineers - (Matchin...</td>\n",
       "      <td>Kyruus - Boston, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stackoverflow.com/jobs/204099/senior-p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11082018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1w ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>s-ID-10</td>\n",
       "      <td>Technical Lead - Lead small, backend teams bui...</td>\n",
       "      <td>HubSpot - Boston, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stackoverflow.com/jobs/206998/technica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11082018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1w ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>s-ID-11</td>\n",
       "      <td>Senior Backend Python/Docker (SQLAlchemy, NoSQ...</td>\n",
       "      <td>FineTune Learning - Boston, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stackoverflow.com/jobs/163285/senior-b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11082018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1w ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>s-ID-12</td>\n",
       "      <td>Senior Software Engineer</td>\n",
       "      <td>Carbon Black - Boston, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stackoverflow.com/jobs/200954/senior-s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11082018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2w ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>s-ID-13</td>\n",
       "      <td>Game Developer for Neuroscience</td>\n",
       "      <td>Seung Lab - Boston, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stackoverflow.com/jobs/103508/game-dev...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11082018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2w ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>s-ID-14</td>\n",
       "      <td>Software Engineer in Test</td>\n",
       "      <td>Eagle Investment Systems - Wellesley, MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://stackoverflow.com/jobs/198219/software...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11082018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2w ago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                              title  \\\n",
       "0    L-ID-0                         Technical Delivery Manager   \n",
       "1    L-ID-1  Sr UI Angular 2 Developer - US Citizen/Green c...   \n",
       "2    L-ID-2                           VP of Product Management   \n",
       "3    L-ID-3          Technical Associate I, AgeLab - Cambridge   \n",
       "4    L-ID-4                                  Product Manager -   \n",
       "5    L-ID-5                                         Engineer l   \n",
       "6    L-ID-6                                   Senior Tech Lead   \n",
       "7    L-ID-7                           Healthcare Data Engineer   \n",
       "8    L-ID-8  QA Automation Engineer w/ Innovative Marketing...   \n",
       "9    L-ID-9  Senior Sales Operations Planning Analyst* - Fa...   \n",
       "10  L-ID-10             Lead Data Manager- Home Based Contract   \n",
       "11  L-ID-11                          Product Marketing Manager   \n",
       "12  L-ID-12                         Director of Data Science -   \n",
       "13  L-ID-13                             Java Engineer - Boston   \n",
       "14  L-ID-14                     Capital Assets Program Manager   \n",
       "0    i-ID-0                  Director of Operations & Security   \n",
       "1    i-ID-1                       Sr Backend Engineer (Python)   \n",
       "2    i-ID-2                                      Data Engineer   \n",
       "3    i-ID-3     INTERNSHIP: R&I Reporting and Analytics Intern   \n",
       "4    i-ID-4                                 Sr. Data Scientist   \n",
       "5    i-ID-5                        Principal Big Data Engineer   \n",
       "6    i-ID-6  Lead software engineer in cloud-based data man...   \n",
       "7    i-ID-7                          Sr. Data Science Engineer   \n",
       "8    i-ID-8                    Principal Data Science Engineer   \n",
       "9    i-ID-9                                  Language Engineer   \n",
       "0   i-ID-10                              NLP Applied Scientist   \n",
       "1   i-ID-11                Scientist I, Biomedical Informatics   \n",
       "2   i-ID-12                Consultant - Supply Chain Analytics   \n",
       "3   i-ID-13  Manager, Software Development – Alexa – Cambri...   \n",
       "4   i-ID-14                                      Data Engineer   \n",
       "0    s-ID-0  Senior/Principal Software Engineers (Backend/D...   \n",
       "1    s-ID-1                                   Systems Engineer   \n",
       "2    s-ID-2                                      Data Engineer   \n",
       "3    s-ID-3                             Deep Learning Engineer   \n",
       "4    s-ID-4                                   Python Developer   \n",
       "5    s-ID-5                           Senior Software Engineer   \n",
       "6    s-ID-6                               Senior Data Engineer   \n",
       "7    s-ID-7              Senior Software Engineer, Backend/API   \n",
       "8    s-ID-8          Software Engineering Manager, Backend/API   \n",
       "9    s-ID-9  Senior/Principal Software Engineers - (Matchin...   \n",
       "10  s-ID-10  Technical Lead - Lead small, backend teams bui...   \n",
       "11  s-ID-11  Senior Backend Python/Docker (SQLAlchemy, NoSQ...   \n",
       "12  s-ID-12                           Senior Software Engineer   \n",
       "13  s-ID-13                    Game Developer for Neuroscience   \n",
       "14  s-ID-14                          Software Engineer in Test   \n",
       "\n",
       "                                      company  \\\n",
       "0                                      Appian   \n",
       "1                              Financial firm   \n",
       "2                                Quest Groups   \n",
       "3       Massachusetts Institute of Technology   \n",
       "4                                Casenet, LLC   \n",
       "5                            Weston & Sampson   \n",
       "6                                      Trianz   \n",
       "7                             iSpecimen, Inc.   \n",
       "8                       Workbridge Associates   \n",
       "9                            Acushnet Company   \n",
       "10                           Premier Research   \n",
       "11                                  HireMinds   \n",
       "12                      Workbridge Associates   \n",
       "13                      Workbridge Associates   \n",
       "14                         Harvard University   \n",
       "0                                    SessionM   \n",
       "1               Strategic Employment Partners   \n",
       "2                                    XL Fleet   \n",
       "3                                        TIAA   \n",
       "4                         Celgene Corporation   \n",
       "5                        Fidelity Investments   \n",
       "6                                        hits   \n",
       "7                                     Staples   \n",
       "8                                     Staples   \n",
       "9                                  Amazon.com   \n",
       "0                                  Amazon.com   \n",
       "1                                      Biogen   \n",
       "2                          McKinsey & Company   \n",
       "3                                  Amazon.com   \n",
       "4                                  Amazon.com   \n",
       "0                         Kyruus - Boston, MA   \n",
       "1   Amadeus North America, Inc. - Waltham, MA   \n",
       "2                       REsurety - Boston, MA   \n",
       "3                       DeepCure - Boston, MA   \n",
       "4               Mide Technology - Medford, MA   \n",
       "5          Vista Higher Learning - Boston, MA   \n",
       "6                    CarGurus - Cambridge, MA   \n",
       "7                         PathAI - Boston, MA   \n",
       "8                         PathAI - Boston, MA   \n",
       "9                         Kyruus - Boston, MA   \n",
       "10                       HubSpot - Boston, MA   \n",
       "11             FineTune Learning - Boston, MA   \n",
       "12                  Carbon Black - Boston, MA   \n",
       "13                     Seung Lab - Boston, MA   \n",
       "14   Eagle Investment Systems - Wellesley, MA   \n",
       "\n",
       "                                   location  \\\n",
       "0                            Boston, MA, US   \n",
       "1                            Boston, MA, US   \n",
       "2                       Greater Boston Area   \n",
       "3                         Cambridge, MA, US   \n",
       "4                           Bedford, MA, US   \n",
       "5                        Foxborough, MA, US   \n",
       "6                        Manchester, NH, US   \n",
       "7                            Boston, MA, US   \n",
       "8                            Boston, MA, US   \n",
       "9                         Fairhaven, MA, US   \n",
       "10                            United States   \n",
       "11                      Greater Boston Area   \n",
       "12                           Boston, MA, US   \n",
       "13                           Boston, MA, US   \n",
       "14  Cambridge, Massachusetts, United States   \n",
       "0                          Boston, MA 02210   \n",
       "1                             Cambridge, MA   \n",
       "2                        Brighton, MA 02135   \n",
       "3                               Waltham, MA   \n",
       "4                             Cambridge, MA   \n",
       "5                          Boston, MA 02212   \n",
       "6                          Boston, MA 02115   \n",
       "7                      Framingham, MA 01702   \n",
       "8                      Framingham, MA 01702   \n",
       "9                                Boston, MA   \n",
       "0                                Boston, MA   \n",
       "1                             Cambridge, MA   \n",
       "2                                Boston, MA   \n",
       "3                             Cambridge, MA   \n",
       "4                         North Reading, MA   \n",
       "0                                       NaN   \n",
       "1                                       NaN   \n",
       "2                                       NaN   \n",
       "3                                       NaN   \n",
       "4                                       NaN   \n",
       "5                                       NaN   \n",
       "6                                       NaN   \n",
       "7                                       NaN   \n",
       "8                                       NaN   \n",
       "9                                       NaN   \n",
       "10                                      NaN   \n",
       "11                                      NaN   \n",
       "12                                      NaN   \n",
       "13                                      NaN   \n",
       "14                                      NaN   \n",
       "\n",
       "                                            post_link  \\\n",
       "0   https://www.linkedin.com/jobs/view/technical-d...   \n",
       "1   https://www.linkedin.com/jobs/view/sr-ui-angul...   \n",
       "2   https://www.linkedin.com/jobs/view/vp-of-produ...   \n",
       "3   https://www.linkedin.com/jobs/view/technical-a...   \n",
       "4   https://www.linkedin.com/jobs/view/product-man...   \n",
       "5   https://www.linkedin.com/jobs/view/engineer-l-...   \n",
       "6   https://www.linkedin.com/jobs/view/senior-tech...   \n",
       "7   https://www.linkedin.com/jobs/view/healthcare-...   \n",
       "8   https://www.linkedin.com/jobs/view/qa-automati...   \n",
       "9   https://www.linkedin.com/jobs/view/senior-sale...   \n",
       "10  https://www.linkedin.com/jobs/view/lead-data-m...   \n",
       "11  https://www.linkedin.com/jobs/view/product-mar...   \n",
       "12  https://www.linkedin.com/jobs/view/director-of...   \n",
       "13  https://www.linkedin.com/jobs/view/java-engine...   \n",
       "14  https://www.linkedin.com/jobs/view/capital-ass...   \n",
       "0   https://www.indeed.com/rc/clk?jk=83ed9b84acbaa...   \n",
       "1   https://www.indeed.com/rc/clk?jk=1a3b2a17d3718...   \n",
       "2   https://www.indeed.com/rc/clk?jk=5a560fd3d8cec...   \n",
       "3   https://www.indeed.com/rc/clk?jk=6a679ac30cded...   \n",
       "4   https://www.indeed.com/rc/clk?jk=ac24bedf9e2a3...   \n",
       "5   https://www.indeed.com/rc/clk?jk=01e8cdaf09531...   \n",
       "6   https://www.indeed.com/rc/clk?jk=d5a7067002a0d...   \n",
       "7   https://www.indeed.com/rc/clk?jk=6db347a6afe08...   \n",
       "8   https://www.indeed.com/rc/clk?jk=21f4b09968b51...   \n",
       "9   https://www.indeed.com/rc/clk?jk=04f5f15d9d04b...   \n",
       "0   https://www.indeed.com/rc/clk?jk=50b51e203d80d...   \n",
       "1   https://www.indeed.com/rc/clk?jk=2c1ed88e515b9...   \n",
       "2   https://www.indeed.com/rc/clk?jk=d7e575de92ecc...   \n",
       "3   https://www.indeed.com/rc/clk?jk=1fe6e6f85af19...   \n",
       "4   https://www.indeed.com/rc/clk?jk=42650568a6466...   \n",
       "0   https://stackoverflow.com/jobs/204101/senior-p...   \n",
       "1   https://stackoverflow.com/jobs/207582/systems-...   \n",
       "2   https://stackoverflow.com/jobs/201669/data-eng...   \n",
       "3   https://stackoverflow.com/jobs/207253/deep-lea...   \n",
       "4   https://stackoverflow.com/jobs/204413/python-d...   \n",
       "5   https://stackoverflow.com/jobs/207206/senior-s...   \n",
       "6   https://stackoverflow.com/jobs/204279/senior-d...   \n",
       "7   https://stackoverflow.com/jobs/169118/senior-s...   \n",
       "8   https://stackoverflow.com/jobs/169119/software...   \n",
       "9   https://stackoverflow.com/jobs/204099/senior-p...   \n",
       "10  https://stackoverflow.com/jobs/206998/technica...   \n",
       "11  https://stackoverflow.com/jobs/163285/senior-b...   \n",
       "12  https://stackoverflow.com/jobs/200954/senior-s...   \n",
       "13  https://stackoverflow.com/jobs/103508/game-dev...   \n",
       "14  https://stackoverflow.com/jobs/198219/software...   \n",
       "\n",
       "                                            comp_link     ezapply  \\\n",
       "0   https://www.linkedin.com/jobs?trk=jobs_chrome_...               \n",
       "1   https://www.linkedin.com/jobs?trk=jobs_chrome_...  Easy Apply   \n",
       "2   https://www.linkedin.com/jobs?trk=jobs_chrome_...  Easy Apply   \n",
       "3   https://www.linkedin.com/jobs?trk=jobs_chrome_...               \n",
       "4   https://www.linkedin.com/jobs?trk=jobs_chrome_...               \n",
       "5   https://www.linkedin.com/jobs?trk=jobs_chrome_...               \n",
       "6   https://www.linkedin.com/jobs?trk=jobs_chrome_...               \n",
       "7   https://www.linkedin.com/jobs?trk=jobs_chrome_...               \n",
       "8   https://www.linkedin.com/jobs?trk=jobs_chrome_...               \n",
       "9   https://www.linkedin.com/jobs?trk=jobs_chrome_...               \n",
       "10  https://www.linkedin.com/jobs?trk=jobs_chrome_...               \n",
       "11  https://www.linkedin.com/jobs?trk=jobs_chrome_...               \n",
       "12  https://www.linkedin.com/jobs?trk=jobs_chrome_...               \n",
       "13  https://www.linkedin.com/jobs?trk=jobs_chrome_...               \n",
       "14  https://www.linkedin.com/jobs?trk=jobs_chrome_...               \n",
       "0           https://www.indeed.com/cmp/SessionM,-Inc.         NaN   \n",
       "1   https://www.indeed.com/cmp/Strategic-Employmen...         NaN   \n",
       "2                 https://www.indeed.com/cmp/XL-Fleet         NaN   \n",
       "3                     https://www.indeed.com/cmp/TIAA         NaN   \n",
       "4                  https://www.indeed.com/cmp/Celgene         NaN   \n",
       "5     https://www.indeed.com/cmp/Fidelity-Investments         NaN   \n",
       "6                     https://www.indeed.com/cmp/HiTS         NaN   \n",
       "7                  https://www.indeed.com/cmp/Staples         NaN   \n",
       "8                  https://www.indeed.com/cmp/Staples         NaN   \n",
       "9               https://www.indeed.com/cmp/Amazon.com         NaN   \n",
       "0               https://www.indeed.com/cmp/Amazon.com         NaN   \n",
       "1                   https://www.indeed.com/cmp/Biogen         NaN   \n",
       "2       https://www.indeed.com/cmp/McKinsey-&-Company         NaN   \n",
       "3               https://www.indeed.com/cmp/Amazon.com         NaN   \n",
       "4               https://www.indeed.com/cmp/Amazon.com         NaN   \n",
       "0                                                 NaN         NaN   \n",
       "1                                                 NaN         NaN   \n",
       "2                                                 NaN         NaN   \n",
       "3                                                 NaN         NaN   \n",
       "4                                                 NaN         NaN   \n",
       "5                                                 NaN         NaN   \n",
       "6                                                 NaN         NaN   \n",
       "7                                                 NaN         NaN   \n",
       "8                                                 NaN         NaN   \n",
       "9                                                 NaN         NaN   \n",
       "10                                                NaN         NaN   \n",
       "11                                                NaN         NaN   \n",
       "12                                                NaN         NaN   \n",
       "13                                                NaN         NaN   \n",
       "14                                                NaN         NaN   \n",
       "\n",
       "                                          company_pic  pulldate  \\\n",
       "0   https://www.linkedin.com/scds/common/u/images/...  11082018   \n",
       "1   https://www.linkedin.com/scds/common/u/images/...  11082018   \n",
       "2   https://www.linkedin.com/scds/common/u/images/...  11082018   \n",
       "3   https://media.licdn.com/dms/image/C4E0BAQHBK7q...  11082018   \n",
       "4                                                None  11082018   \n",
       "5   https://media.licdn.com/dms/image/C4D0BAQFbppI...  11082018   \n",
       "6                                                None  11082018   \n",
       "7                                                None  11082018   \n",
       "8                                                None  11082018   \n",
       "9   https://media.licdn.com/dms/image/C4E0BAQE8M_T...  11082018   \n",
       "10  https://media.licdn.com/dms/image/C560BAQHEqcj...  11082018   \n",
       "11  https://media.licdn.com/dms/image/C510BAQHVUFQ...  11082018   \n",
       "12                                               None  11082018   \n",
       "13                                               None  11082018   \n",
       "14  https://media.licdn.com/dms/image/C4E0BAQF5t62...  11082018   \n",
       "0                                                  NA  11082018   \n",
       "1   https://d2q79iu7y748jz.cloudfront.net/s/_logo/...  11082018   \n",
       "2                                                  NA  11082018   \n",
       "3   https://d2q79iu7y748jz.cloudfront.net/s/_logo/...  11082018   \n",
       "4   https://d2q79iu7y748jz.cloudfront.net/s/_logo/...  11082018   \n",
       "5   https://d2q79iu7y748jz.cloudfront.net/s/_logo/...  11082018   \n",
       "6                                                  NA  11082018   \n",
       "7   https://d2q79iu7y748jz.cloudfront.net/s/_logo/...  11082018   \n",
       "8   https://d2q79iu7y748jz.cloudfront.net/s/_logo/...  11082018   \n",
       "9   https://d2q79iu7y748jz.cloudfront.net/s/_logo/...  11082018   \n",
       "0   https://d2q79iu7y748jz.cloudfront.net/s/_logo/...  11082018   \n",
       "1   https://d2q79iu7y748jz.cloudfront.net/s/_logo/...  11082018   \n",
       "2   https://d2q79iu7y748jz.cloudfront.net/s/_logo/...  11082018   \n",
       "3   https://d2q79iu7y748jz.cloudfront.net/s/_logo/...  11082018   \n",
       "4   https://d2q79iu7y748jz.cloudfront.net/s/_logo/...  11082018   \n",
       "0                                                 NaN  11082018   \n",
       "1                                                 NaN  11082018   \n",
       "2                                                 NaN  11082018   \n",
       "3                                                 NaN  11082018   \n",
       "4                                                 NaN  11082018   \n",
       "5                                                 NaN  11082018   \n",
       "6                                                 NaN  11082018   \n",
       "7                                                 NaN  11082018   \n",
       "8                                                 NaN  11082018   \n",
       "9                                                 NaN  11082018   \n",
       "10                                                NaN  11082018   \n",
       "11                                                NaN  11082018   \n",
       "12                                                NaN  11082018   \n",
       "13                                                NaN  11082018   \n",
       "14                                                NaN  11082018   \n",
       "\n",
       "                                             comp_loc       rating  \\\n",
       "0                              Appian, Boston, MA, US          NaN   \n",
       "1                      Financial firm, Boston, MA, US          NaN   \n",
       "2                   Quest Groups, Greater Boston Area          NaN   \n",
       "3   Massachusetts Institute of Technology, Cambrid...          NaN   \n",
       "4                       Casenet, LLC, Bedford, MA, US          NaN   \n",
       "5                Weston & Sampson, Foxborough, MA, US          NaN   \n",
       "6                          Trianz, Manchester, NH, US          NaN   \n",
       "7                     iSpecimen, Inc., Boston, MA, US          NaN   \n",
       "8               Workbridge Associates, Boston, MA, US          NaN   \n",
       "9                 Acushnet Company, Fairhaven, MA, US          NaN   \n",
       "10                    Premier Research, United States          NaN   \n",
       "11                     HireMinds, Greater Boston Area          NaN   \n",
       "12              Workbridge Associates, Boston, MA, US          NaN   \n",
       "13              Workbridge Associates, Boston, MA, US          NaN   \n",
       "14  Harvard University, Cambridge, Massachusetts, ...          NaN   \n",
       "0                    SessionM, Inc., Boston, MA 02210          3.7   \n",
       "1        Strategic Employment Partners, Cambridge, MA          5.0   \n",
       "2                        XL Fleet, Brighton, MA 02135  Not ranked.   \n",
       "3                                   TIAA, Waltham, MA          3.7   \n",
       "4                              Celgene, Cambridge, MA          4.0   \n",
       "5              Fidelity Investments, Boston, MA 02212          4.1   \n",
       "6                              HiTS, Boston, MA 02115          3.9   \n",
       "7                       Staples, Framingham, MA 01702          3.6   \n",
       "8                       Staples, Framingham, MA 01702          3.6   \n",
       "9                              Amazon.com, Boston, MA          3.6   \n",
       "0                              Amazon.com, Boston, MA          3.6   \n",
       "1                               Biogen, Cambridge, MA          4.1   \n",
       "2                      McKinsey & Company, Boston, MA          4.3   \n",
       "3                           Amazon.com, Cambridge, MA          3.6   \n",
       "4                       Amazon.com, North Reading, MA          3.6   \n",
       "0                                                 NaN          NaN   \n",
       "1                                                 NaN          NaN   \n",
       "2                                                 NaN          NaN   \n",
       "3                                                 NaN          NaN   \n",
       "4                                                 NaN          NaN   \n",
       "5                                                 NaN          NaN   \n",
       "6                                                 NaN          NaN   \n",
       "7                                                 NaN          NaN   \n",
       "8                                                 NaN          NaN   \n",
       "9                                                 NaN          NaN   \n",
       "10                                                NaN          NaN   \n",
       "11                                                NaN          NaN   \n",
       "12                                                NaN          NaN   \n",
       "13                                                NaN          NaN   \n",
       "14                                                NaN          NaN   \n",
       "\n",
       "                                        salaries_link  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "0       https://www.indeed.com/cmp/SessionM/salaries/   \n",
       "1   https://www.indeed.com/cmp/Strategic Employmen...   \n",
       "2       https://www.indeed.com/cmp/XL Fleet/salaries/   \n",
       "3           https://www.indeed.com/cmp/TIAA/salaries/   \n",
       "4   https://www.indeed.com/cmp/Celgene Corporation...   \n",
       "5   https://www.indeed.com/cmp/Fidelity Investment...   \n",
       "6           https://www.indeed.com/cmp/hits/salaries/   \n",
       "7        https://www.indeed.com/cmp/Staples/salaries/   \n",
       "8        https://www.indeed.com/cmp/Staples/salaries/   \n",
       "9     https://www.indeed.com/cmp/Amazon.com/salaries/   \n",
       "0     https://www.indeed.com/cmp/Amazon.com/salaries/   \n",
       "1         https://www.indeed.com/cmp/Biogen/salaries/   \n",
       "2   https://www.indeed.com/cmp/McKinsey & Company/...   \n",
       "3     https://www.indeed.com/cmp/Amazon.com/salaries/   \n",
       "4     https://www.indeed.com/cmp/Amazon.com/salaries/   \n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "\n",
       "                                         reviews_link  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "0        https://www.indeed.com/cmp/SessionM/reviews/   \n",
       "1   https://www.indeed.com/cmp/Strategic Employmen...   \n",
       "2        https://www.indeed.com/cmp/XL Fleet/reviews/   \n",
       "3            https://www.indeed.com/cmp/TIAA/reviews/   \n",
       "4   https://www.indeed.com/cmp/Celgene Corporation...   \n",
       "5   https://www.indeed.com/cmp/Fidelity Investment...   \n",
       "6            https://www.indeed.com/cmp/hits/reviews/   \n",
       "7         https://www.indeed.com/cmp/Staples/reviews/   \n",
       "8         https://www.indeed.com/cmp/Staples/reviews/   \n",
       "9      https://www.indeed.com/cmp/Amazon.com/reviews/   \n",
       "0      https://www.indeed.com/cmp/Amazon.com/reviews/   \n",
       "1          https://www.indeed.com/cmp/Biogen/reviews/   \n",
       "2   https://www.indeed.com/cmp/McKinsey & Company/...   \n",
       "3      https://www.indeed.com/cmp/Amazon.com/reviews/   \n",
       "4      https://www.indeed.com/cmp/Amazon.com/reviews/   \n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "\n",
       "                        l_company     postdate  \\\n",
       "0                             NaN          NaN   \n",
       "1                             NaN          NaN   \n",
       "2                             NaN          NaN   \n",
       "3                             NaN          NaN   \n",
       "4                             NaN          NaN   \n",
       "5                             NaN          NaN   \n",
       "6                             NaN          NaN   \n",
       "7                             NaN          NaN   \n",
       "8                             NaN          NaN   \n",
       "9                             NaN          NaN   \n",
       "10                            NaN          NaN   \n",
       "11                            NaN          NaN   \n",
       "12                            NaN          NaN   \n",
       "13                            NaN          NaN   \n",
       "14                            NaN          NaN   \n",
       "0                        sessionm  5 hours ago   \n",
       "1   strategic employment partners  3 hours ago   \n",
       "2                        xl fleet  5 hours ago   \n",
       "3                            tiaa  4 hours ago   \n",
       "4             celgene corporation  4 hours ago   \n",
       "5            fidelity investments  5 hours ago   \n",
       "6                            hits  5 hours ago   \n",
       "7                         staples  4 hours ago   \n",
       "8                         staples  4 hours ago   \n",
       "9                      amazon.com  7 hours ago   \n",
       "0                      amazon.com  7 hours ago   \n",
       "1                          biogen  6 hours ago   \n",
       "2              mckinsey & company  9 hours ago   \n",
       "3                      amazon.com  8 hours ago   \n",
       "4                      amazon.com  7 hours ago   \n",
       "0                             NaN       1w ago   \n",
       "1                             NaN      20h ago   \n",
       "2                             NaN       3d ago   \n",
       "3                             NaN       5d ago   \n",
       "4                             NaN       5d ago   \n",
       "5                             NaN       6d ago   \n",
       "6                             NaN       6d ago   \n",
       "7                             NaN       1w ago   \n",
       "8                             NaN       1w ago   \n",
       "9                             NaN       1w ago   \n",
       "10                            NaN       1w ago   \n",
       "11                            NaN       1w ago   \n",
       "12                            NaN       2w ago   \n",
       "13                            NaN       2w ago   \n",
       "14                            NaN       2w ago   \n",
       "\n",
       "                                            body_text  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "0   Contract\\nAs the Director of Operations & Secu...   \n",
       "1   $120,000 a year\\nSenior Backend Engineer (Pyth...   \n",
       "2   XL seeks an experienced Data Engineer to work ...   \n",
       "3   Internship\\nJob Description\\nCOMPANY OVERVIEW:...   \n",
       "4   Other Locations: US- MA- Cambridge\\n\\nDescript...   \n",
       "5   Fidelity Personal Investing has an opportunity...   \n",
       "6   About the position\\nThe Harvard Medical School...   \n",
       "7   Description\\nWe are looking for an experienced...   \n",
       "8   Description\\nWe are looking for an experienced...   \n",
       "9   Job Description\\n\\nInterested in Amazon Echo? ...   \n",
       "0   Job Description\\nInterested in Amazon Echo and...   \n",
       "1   Effective use of disruptive technologies is at...   \n",
       "2   Qualifications\\nMBA or advanced degree in Oper...   \n",
       "3   Job Description\\nDo you want to work on Alexa,...   \n",
       "4   Job Description\\nAre you inspired by invention...   \n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "\n",
       "                                              summary  \n",
       "0                                                 NaN  \n",
       "1                                                 NaN  \n",
       "2                                                 NaN  \n",
       "3                                                 NaN  \n",
       "4                                                 NaN  \n",
       "5                                                 NaN  \n",
       "6                                                 NaN  \n",
       "7                                                 NaN  \n",
       "8                                                 NaN  \n",
       "9                                                 NaN  \n",
       "10                                                NaN  \n",
       "11                                                NaN  \n",
       "12                                                NaN  \n",
       "13                                                NaN  \n",
       "14                                                NaN  \n",
       "0   Highly engaged technologist with broad experie...  \n",
       "1   5+ years working with Python professionally. W...  \n",
       "2   Including object-oriented Python as well as fu...  \n",
       "3   Experience with programming language (SQL, SAS...  \n",
       "4   Excellent skills in R programming and experien...  \n",
       "5   Your expertise in languages such as Python, Ru...  \n",
       "6   Python, JavaScript and Java; High fluency in m...  \n",
       "7   With at least a working knowledge of Python. P...  \n",
       "8   With at least a working knowledge of Python. E...  \n",
       "9   Experience in Perl, Python, or another scripti...  \n",
       "0   Expert in at least one more major programming ...  \n",
       "1   Expertise in developing, using or adapting ope...  \n",
       "2   R, Python, SAS, SQL, Tableau, etc. MBA or adva...  \n",
       "3   Experience with one or more modern programming...  \n",
       "4   Proficiency in, at least, one modern programmi...  \n",
       "0                                                 NaN  \n",
       "1                                                 NaN  \n",
       "2                                                 NaN  \n",
       "3                                                 NaN  \n",
       "4                                                 NaN  \n",
       "5                                                 NaN  \n",
       "6                                                 NaN  \n",
       "7                                                 NaN  \n",
       "8                                                 NaN  \n",
       "9                                                 NaN  \n",
       "10                                                NaN  \n",
       "11                                                NaN  \n",
       "12                                                NaN  \n",
       "13                                                NaN  \n",
       "14                                                NaN  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch = ult_scrap(**{\n",
    "    'results_per_site' : 15,\n",
    "    'job_title' : 'python',\n",
    "    'city' : 'boston',\n",
    "    'site_list' : ['linkedin', 'stack', 'indeed'],\n",
    "    'linked_in_username' : 'YOURUSERNAMEHERE',\n",
    "    'linked_in_password' : 'YOURPASSWORD',\n",
    "    'chromedriver_location' : u'YOURCHROMEDRIVIERLOC',\n",
    "    'api_key' : 'YOURGOOGLEMAPSAPIKEY'\n",
    "})\n",
    "fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for elem in fetch['post_link']:\\n    print(elem)\""
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for elem in fetch['post_link']:\n",
    "    print(elem)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument(\"--disable-extensions\")\n",
    "options.add_argument('--log-level=3')\n",
    "wd = webdriver.Chrome(scrap_params['chromedriver_location'], options=options)\n",
    "url = 'https://www.indeed.com/jobs?q=python&l=worcester&start=0&sort=date&radius=25'\n",
    "wd.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/jobs?q=python&l=worcester&start=0&sort=date&radius=25\n",
      "10 \n",
      " 10 \n",
      " 10 \n",
      " 10 \n",
      " 10\n",
      "-----------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------\n",
      "                        company       rating  \\\n",
      "0                  Diverse Lynx          4.8   \n",
      "1                   PerkinElmer          3.8   \n",
      "2                       Marvell          3.4   \n",
      "3                       Marvell          3.4   \n",
      "4                          Bose          4.2   \n",
      "5                          Bose          4.2   \n",
      "6                          Bose          4.2   \n",
      "7                          Bose          4.2   \n",
      "8                       Marvell          3.4   \n",
      "9                       Marvell          3.4   \n",
      "10                         Dell          4.0   \n",
      "11                        Intel          4.1   \n",
      "12  The Hanover Insurance Group          3.8   \n",
      "13           Spill Center, Inc.  Not ranked.   \n",
      "\n",
      "                                        salaries_link  \\\n",
      "0   https://www.indeed.com/cmp/Diverse Lynx/salaries/   \n",
      "1    https://www.indeed.com/cmp/PerkinElmer/salaries/   \n",
      "2        https://www.indeed.com/cmp/Marvell/salaries/   \n",
      "3        https://www.indeed.com/cmp/Marvell/salaries/   \n",
      "4           https://www.indeed.com/cmp/Bose/salaries/   \n",
      "5           https://www.indeed.com/cmp/Bose/salaries/   \n",
      "6           https://www.indeed.com/cmp/Bose/salaries/   \n",
      "7           https://www.indeed.com/cmp/Bose/salaries/   \n",
      "8        https://www.indeed.com/cmp/Marvell/salaries/   \n",
      "9        https://www.indeed.com/cmp/Marvell/salaries/   \n",
      "10          https://www.indeed.com/cmp/Dell/salaries/   \n",
      "11         https://www.indeed.com/cmp/Intel/salaries/   \n",
      "12  https://www.indeed.com/cmp/The Hanover Insuran...   \n",
      "13  https://www.indeed.com/cmp/Spill Center, Inc./...   \n",
      "\n",
      "                                         reviews_link  \\\n",
      "0    https://www.indeed.com/cmp/Diverse Lynx/reviews/   \n",
      "1     https://www.indeed.com/cmp/PerkinElmer/reviews/   \n",
      "2         https://www.indeed.com/cmp/Marvell/reviews/   \n",
      "3         https://www.indeed.com/cmp/Marvell/reviews/   \n",
      "4            https://www.indeed.com/cmp/Bose/reviews/   \n",
      "5            https://www.indeed.com/cmp/Bose/reviews/   \n",
      "6            https://www.indeed.com/cmp/Bose/reviews/   \n",
      "7            https://www.indeed.com/cmp/Bose/reviews/   \n",
      "8         https://www.indeed.com/cmp/Marvell/reviews/   \n",
      "9         https://www.indeed.com/cmp/Marvell/reviews/   \n",
      "10           https://www.indeed.com/cmp/Dell/reviews/   \n",
      "11          https://www.indeed.com/cmp/Intel/reviews/   \n",
      "12  https://www.indeed.com/cmp/The Hanover Insuran...   \n",
      "13  https://www.indeed.com/cmp/Spill Center, Inc./...   \n",
      "\n",
      "                                             title      posttime  \\\n",
      "0                                     Hadoop Admin   8 hours ago   \n",
      "1                        Senior Software Developer  17 hours ago   \n",
      "2                                RTL Design Intern     1 day ago   \n",
      "3                   Intern - Verification Engineer    3 days ago   \n",
      "4                               Sr. Data Scientist     1 day ago   \n",
      "5                    Senior QA Automation Engineer    2 days ago   \n",
      "6                               Sr. Data Scientist     1 day ago   \n",
      "7                    Senior QA Automation Engineer    2 days ago   \n",
      "8                                RTL Design Intern     1 day ago   \n",
      "9                   Intern - Verification Engineer    3 days ago   \n",
      "10                                             NaN           NaN   \n",
      "11  Validation Infrastructure Development Engineer    3 days ago   \n",
      "12                        Associate Data Scientist    4 days ago   \n",
      "13                   Junior Applications Developer    4 days ago   \n",
      "\n",
      "                 location                                        company_pic  \\\n",
      "0          Woonsocket, RI  https://d2q79iu7y748jz.cloudfront.net/s/_squar...   \n",
      "1           Hopkinton, MA  https://d2q79iu7y748jz.cloudfront.net/s/_logo/...   \n",
      "2   Marlborough, MA 01752  https://d2q79iu7y748jz.cloudfront.net/s/_logo/...   \n",
      "3   Marlborough, MA 01752  https://d2q79iu7y748jz.cloudfront.net/s/_logo/...   \n",
      "4    Framingham, MA 01701  https://d2q79iu7y748jz.cloudfront.net/s/_logo/...   \n",
      "5    Framingham, MA 01701  https://d2q79iu7y748jz.cloudfront.net/s/_logo/...   \n",
      "6    Framingham, MA 01701  https://d2q79iu7y748jz.cloudfront.net/s/_logo/...   \n",
      "7    Framingham, MA 01701  https://d2q79iu7y748jz.cloudfront.net/s/_logo/...   \n",
      "8   Marlborough, MA 01752  https://d2q79iu7y748jz.cloudfront.net/s/_logo/...   \n",
      "9   Marlborough, MA 01752  https://d2q79iu7y748jz.cloudfront.net/s/_logo/...   \n",
      "10                    NaN                                                NaN   \n",
      "11       Hudson, MA 01749  https://d2q79iu7y748jz.cloudfront.net/s/_logo/...   \n",
      "12    Worcester, MA 01653  https://d2q79iu7y748jz.cloudfront.net/s/_logo/...   \n",
      "13       Hudson, MA 01749                                                 NA   \n",
      "\n",
      "                                            body_text  sID    site  \\\n",
      "0   Contract\\nJob Title: Hadoop Admin\\n\\nJob Locat...  sID  indeed   \n",
      "1   Responsibilities:\\nParticipate in the design, ...  sID  indeed   \n",
      "2   Internship\\nRTL DESIGNER - INTERN\\n\\nContribut...  sID  indeed   \n",
      "3   Internship\\nResponsibilities:\\nWriting program...  sID  indeed   \n",
      "4   Data science group in Consumer Electronics bus...  sID  indeed   \n",
      "5   Think you know Bose? Look closer.\\n\\nWe’ve spe...  sID  indeed   \n",
      "6   Data science group in Consumer Electronics bus...  sID  indeed   \n",
      "7   Think you know Bose? Look closer.\\n\\nWe’ve spe...  sID  indeed   \n",
      "8   Internship\\nRTL DESIGNER - INTERN\\n\\nContribut...  sID  indeed   \n",
      "9   Internship\\nResponsibilities:\\nWriting program...  sID  indeed   \n",
      "10                                                NaN  NaN     NaN   \n",
      "11  Job Description\\nJob DescriptionAre you ready ...  sID  indeed   \n",
      "12  Overview\\n\\nOur Data Science department is see...  sID  indeed   \n",
      "13  Our Company\\n\\nSpill Center is an environmenta...  sID  indeed   \n",
      "\n",
      "                                              summary  \\\n",
      "0   Python, shell scripting, SQL (preferably Terad...   \n",
      "1   Fluency in C# and at least one additional prog...   \n",
      "2   RTL DESIGNER - INTERN Contribute as a design e...   \n",
      "3   Some experience with scripting languages like ...   \n",
      "4   Proficiency in data science software developme...   \n",
      "5   Mastery of Python, Perl, JavaScript, or simila...   \n",
      "6   Proficiency in data science software developme...   \n",
      "7   Mastery of Python, Perl, JavaScript, or simila...   \n",
      "8   RTL DESIGNER - INTERN Contribute as a design e...   \n",
      "9   Some experience with scripting languages like ...   \n",
      "10                                                NaN   \n",
      "11  Perl, Python, or other scripting languages exp...   \n",
      "12  SAS, SQL, VBA, R, Python. Our Data Science dep...   \n",
      "13  Ability to program in at least one programming...   \n",
      "\n",
      "                                           post_links  \\\n",
      "0   https://www.indeed.com/rc/clk?jk=033c8e70396f9...   \n",
      "1   https://www.indeed.com/rc/clk?jk=b1d1505969f0c...   \n",
      "2   https://www.indeed.com/rc/clk?jk=79655463fc574...   \n",
      "3   https://www.indeed.com/rc/clk?jk=da4f6ebaf1888...   \n",
      "4   https://www.indeed.com/rc/clk?jk=6ea87a474808e...   \n",
      "5   https://www.indeed.com/rc/clk?jk=2d2b7d4aef088...   \n",
      "6   https://www.indeed.com/rc/clk?jk=6ea87a474808e...   \n",
      "7   https://www.indeed.com/rc/clk?jk=2d2b7d4aef088...   \n",
      "8   https://www.indeed.com/rc/clk?jk=79655463fc574...   \n",
      "9   https://www.indeed.com/rc/clk?jk=da4f6ebaf1888...   \n",
      "10                                                NaN   \n",
      "11  https://www.indeed.com/rc/clk?jk=fbc2fffa62c1e...   \n",
      "12  https://www.indeed.com/rc/clk?jk=f762a1ef49af0...   \n",
      "13  https://www.indeed.com/rc/clk?jk=a525724756819...   \n",
      "\n",
      "                                           comp_links  \n",
      "0             https://www.indeed.com/cmp/Diverse-Lynx  \n",
      "1              https://www.indeed.com/cmp/PerkinElmer  \n",
      "2                  https://www.indeed.com/cmp/Marvell  \n",
      "3                  https://www.indeed.com/cmp/Marvell  \n",
      "4                     https://www.indeed.com/cmp/Bose  \n",
      "5                     https://www.indeed.com/cmp/Bose  \n",
      "6                     https://www.indeed.com/cmp/Bose  \n",
      "7                     https://www.indeed.com/cmp/Bose  \n",
      "8                  https://www.indeed.com/cmp/Marvell  \n",
      "9                  https://www.indeed.com/cmp/Marvell  \n",
      "10                                                NaN  \n",
      "11                   https://www.indeed.com/cmp/Intel  \n",
      "12  https://www.indeed.com/cmp/The-Hanover-Insuran...  \n",
      "13      https://www.indeed.com/cmp/Spill-Center,-Inc.  \n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument(\"--disable-extensions\")\n",
    "options.add_argument('--log-level=3')\n",
    "wd = webdriver.Chrome(scrap_params['chromedriver_location'], options=options)\n",
    "url = 'https://www.indeed.com/jobs?q=python&l=worcester&start=0&sort=date&radius=25'\n",
    "print(url)\n",
    "wd.get(url)\n",
    "companies = []\n",
    "\n",
    "#companies = [company.text for company in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']\")]\n",
    "companies = [company.text for company in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//span[@class='company']\")]\n",
    "titles = [title.text for title in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//h2[@class='jobtitle']\")]\n",
    "description = [description.text for description in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//span[@class='summary']\")]\n",
    "post_links = [post_link.get_attribute('href') for post_link in wd.find_elements_by_xpath(\"//div[@data-tn-component='organicJob']//h2[@class='jobtitle']//a\")]\n",
    "comp_links = ['https://www.indeed.com/cmp/' + str(company).replace(' ', '-') for company in companies]\n",
    "\n",
    "tmp_df = pd.DataFrame({'sID' : 'sID', 'site' : 'indeed', 'title' : titles, 'company' : companies, 'summary' : description, 'post_links' : post_links, 'comp_links' : comp_links})\n",
    "\n",
    "comp_list = []\n",
    "for link in comp_links:\n",
    "    wd.get(link)\n",
    "    company_name = wd.find_element_by_xpath(\"//div[@class='cmp-company-name-container']\").text\n",
    "    try:\n",
    "        average_comp_rating = wd.find_element_by_xpath(\"//span[@class='cmp-header-rating-average']\").text\n",
    "    except:\n",
    "        average_comp_rating = 'Not ranked.'\n",
    "    salaries_link = 'https://www.indeed.com/cmp/' + company_name + '/salaries/'\n",
    "    reviews_link = 'https://www.indeed.com/cmp/' + company_name + '/reviews/'\n",
    "    comp_list.append([company_name, average_comp_rating, salaries_link, reviews_link])\n",
    "comp_df = pd.DataFrame(comp_list, columns=['company', 'rating', 'salaries_link', 'reviews_link'])\n",
    "\n",
    "post_list = []\n",
    "for link in post_links:\n",
    "    wd.get(link)\n",
    "    jobtitle = wd.find_element_by_xpath(\"//h3[contains(@class, 'jobsearch-JobInfoHeader-title')]\").text\n",
    "    #jobtitle = wd.find_element_by_xpath(\"//h3[@class='jobsearch-JobInfoHeader-title']\").text\n",
    "    #company_name = wd.find_element_by_xpath(\"//div[@class='icl-u-lg-mr--sm icl-u-xs-mr--xs']\").text\n",
    "    try:\n",
    "        company_pic = wd.find_element_by_xpath(\"//img[@class='jobsearch-CompanyAvatar-image']\").get_attribute('src')\n",
    "    except:\n",
    "        company_pic = 'NA'\n",
    "    line = wd.find_element_by_xpath(\"//div[contains(@class, 'jobsearch-InlineCompanyRating')]\").text\n",
    "    location = line.split('\\n')[2]\n",
    "    company_name = line.split('\\n')[0]\n",
    "    body = wd.find_element_by_xpath(\"//div[contains(@class, 'jobsearch-JobComponent-description')]\").text\n",
    "    post_time = wd.find_element_by_xpath(\"//div[@class='jobsearch-JobMetadataFooter']\").text.split('-')[1].strip()\n",
    "    post_list.append([jobtitle, company_name, post_time, location, company_pic, body])\n",
    "post_df = pd.DataFrame(post_list, columns=['title', 'company', 'posttime', 'location', 'company_pic', 'body_text'])\n",
    "job_df = comp_df.merge(post_df, on='company', how='left')\n",
    "job_df = job_df.merge(tmp_df, on=['company', 'title'], how='left')\n",
    "wd.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              https://www.indeed.com/cmp/Diverse-Lynx\n",
       "1               https://www.indeed.com/cmp/PerkinElmer\n",
       "2                   https://www.indeed.com/cmp/Marvell\n",
       "3                      https://www.indeed.com/cmp/Bose\n",
       "4                      https://www.indeed.com/cmp/Bose\n",
       "5                   https://www.indeed.com/cmp/Marvell\n",
       "6                     https://www.indeed.com/cmp/Intel\n",
       "7                      https://www.indeed.com/cmp/DELL\n",
       "8    https://www.indeed.com/cmp/The-Hanover-Insuran...\n",
       "9        https://www.indeed.com/cmp/Spill-Center,-Inc.\n",
       "Name: comp_links, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_df['comp_links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tex_output(fetch, suffix=''):\n",
    "    fetch_df = fetch\n",
    "    #fetch_dict = fetch[1]\n",
    "    map_list = []\n",
    "    content = r'''\\UseRawInputEncoding%\n",
    "\\documentclass{{article}}%\n",
    "\\usepackage[T1]{{fontenc}}%\n",
    "\\usepackage[utf8]{{inputenc}}%\n",
    "\\inputencoding{utf8}\n",
    "\\usepackage{{lmodern}}%\n",
    "\\usepackage{graphicx}\n",
    "\\usepackage{{textcomp}}%\n",
    "\\usepackage{{lastpage}}%\n",
    "\n",
    "\\begin{document}%\n",
    "\\normalsize%'''\n",
    "    \n",
    "    for index, row in fetch_df.iterrows():\n",
    "        print(index)\n",
    "        print(row['map_url'])\n",
    "        map_file_name = str(index) + '.jpg'\n",
    "        map_list.append(map_file_name)\n",
    "        urllib.request.urlretrieve(str(row['map_url']), map_file_name)\n",
    "        content = content + r'''\n",
    "\\section{{{jobtitle}}}%\n",
    "\\subsection{{{comploc}}}\n",
    "{{{address}}}\n",
    "{{{distance}}}\n",
    "{{{duration}}}\n",
    "\n",
    "\n",
    "\\begin{{figure}}\n",
    "\\includegraphics{{{map_file_name}}}\n",
    "\\end{{figure}}\n",
    "\n",
    "\\subsection{{Salary}}%\n",
    "{{{salary}}}\n",
    "\n",
    "\\subsection{{Link}}%\n",
    "{{{link}}}\n",
    "\n",
    "\\newpage\n",
    "\n",
    "\n",
    "'''.format(**{'jobtitle': row['title'], 'salary' : row['salary'], 'comploc' : row['comp_loc'], 'address' : row['address'], 'distance' : row['distance'], \n",
    "              'duration' : row['duration'], 'link' : row['link'], 'map_file_name' : map_file_name}).replace('&', '\\&').replace('$', '\\$').replace('#', '\\#').replace('|', '').replace('_', '\\_')        \n",
    "        filename = 'job_scrap_output' + suffix + '.tex'\n",
    "\n",
    "    content = content + '''\n",
    "\\end{document}%'''\n",
    "    with open(filename,'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    cmd = ['pdflatex', filename]\n",
    "    subprocess.check_output(cmd)\n",
    "\n",
    "    os.unlink('job_scrap_output.log')\n",
    "    os.unlink('job_scrap_output.aux')\n",
    "    for file in map_list:\n",
    "        os.unlink(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'map_url'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   3123\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3124\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mlibindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value_box\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3125\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_box\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_box\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-910d6a2c0a4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtex_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-152-17e279fbfed2>\u001b[0m in \u001b[0;36mtex_output\u001b[1;34m(fetch, suffix)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetch_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'map_url'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mmap_file_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.jpg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mmap_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   3130\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3131\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3132\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3133\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3134\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   3116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3117\u001b[0m             return self._engine.get_value(s, k,\n\u001b[1;32m-> 3118\u001b[1;33m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[0;32m   3119\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3120\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'integer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'boolean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'map_url'"
     ]
    }
   ],
   "source": [
    "tex_output(fetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here's below\n",
    "\n",
    "'''indeed_pg_num = 0\n",
    "                site_ind = 0 # set to 0 at start of site and compared to results so there are 'results' number of pulls per site\n",
    "                list_o_divs = [] # holds divs if multiple pages\n",
    "                tmp_dict = {} # holds info on each div loop\n",
    "                # https://www.indeed.com/jobs?q=data+science&l=boston&sort=date\n",
    "                res_left = results\n",
    "                base = 'https://www.indeed.com/jobs?'\n",
    "                while res_left > 0:            \n",
    "                    params = {'q' : job_title,'l' : city, 'start' : (results-res_left),  'sort' : 'date', 'radius': '25'}\n",
    "                    page = requests.get(base, params) \n",
    "                    time.sleep(1)  \n",
    "                    soup = get_soup(page.text)\n",
    "                    divs = soup.find_all(name=\"div\", attrs={\"class\":\"row\"})\n",
    "                    list_o_divs.append(divs)\n",
    "                    res_left = res_left - len(divs)\n",
    "                    indeed_pg_num = indeed_pg_num + 1\n",
    "                print('total indeed pages : ' + str(indeed_pg_num))\n",
    "                for divs in list_o_divs:\n",
    "                    for div in divs:\n",
    "                        if (site_ind < results):\n",
    "                            tmp_dict['sID'] = 'ID' + str(num)\n",
    "                            tmp_dict['site'] = site\n",
    "                            tmp_dict['pulldate'] = str(time.strftime(\"%M%D%y\", time.localtime(time.time())))\n",
    "                            tmp_dict['link'] = 'http://www.indeed.com' + div.find(name='a', attrs={'class':'turnstileLink'})['href']\n",
    "                            #tmp_dict['link'] = extract_link(div, site)\n",
    "                            tmp_dict['title'] = div.find(name='a', attrs={'class':'turnstileLink'})['title']\n",
    "                            # tmp_dict['title'] = extract_job_title(div, site)\n",
    "                            tmp_dict['summary'] = div.find('span', attrs={'class': 'summary'}).text.strip()\n",
    "                            # tmp_dict['summary'] = extract_summary(div, site)\n",
    "                            #tmp_dict['salary'] = extract_salary(div, site)\n",
    "                            # tmp_dict['location'] = extract_location(div, site).strip() # two things\n",
    "                            if div.find('span', attrs={'class': 'location'}).text is None:\n",
    "                                tmp_dict['location'] = div.find('div', attrs={'class': 'location'}).text.strip()\n",
    "                            else:\n",
    "                                tmp_dict['location'] = div.find('span', attrs={'class': 'location'}).text.strip()\n",
    "                            tmp_dict['tags'] = extract_tags(div, site)\n",
    "                            #tmp_dict['id'] = extract_id(div, site)\n",
    "                            tmp_dict['id'] = div['id']\n",
    "                            if div.find('div', attrs={'class': 'iaP'}) is not None:\n",
    "                                tmp_dict['ezapply'] = True\n",
    "                            else:\n",
    "                                tmp_dict['ezapply'] = False\n",
    "                            # tmp_dict['ezapply'] = extract_easyapply(div, site)\n",
    "                            tmp_dict['postdate'] = extract_postdate(div, site)\n",
    "                            #tmp_dict['company'] = extract_company(div, site).strip()\n",
    "                            tmp_dict['company'] = div.find(name=\"span\", attrs={\"class\":\"company\"}).text.replace('\\n', '').strip()\n",
    "                            tmp_dict['comp_loc'] = str(extract_company(div, site).strip() + ', ' + extract_location(div, site).strip())\n",
    "                            tmp_df = pd.DataFrame(tmp_dict, index=[tmp_dict['sID']])\n",
    "                            job_df = job_df.append(tmp_df, sort=False)\n",
    "                            num = num + 1\n",
    "                            site_ind = site_ind + 1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
